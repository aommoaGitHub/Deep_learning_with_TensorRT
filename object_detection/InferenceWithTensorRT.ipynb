{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow and TensorRT to Frozen model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.tensorrt as trt\n",
    "from tensorflow.python.platform import gfile\n",
    "import zipfile\n",
    "\n",
    "from distutils.version import StrictVersion\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "# For import utils.\n",
    "sys.path.append(\"..\")\n",
    "from object_detection.utils import ops as utils_ops\n",
    "\n",
    "from utils import label_map_util\n",
    "from utils import visualization_utils as vis_util\n",
    "\n",
    "import pprint\n",
    "import time\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Jetson TX2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "K.set_session(tf.Session(config=config))\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables of models\n",
    "All models are in ./models folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_dir = time.strftime(\"%Y_%m_%d-%H_%M_%S\", time.localtime())\n",
    "print(sub_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1) ssd_mobilenet_v1_coco_2017_11_17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What model to download.\n",
    "MODEL_NAME = './models/ssd_mobilenet_v1_coco_2017_11_17'\n",
    "MODEL_FILE = MODEL_NAME + '.tar.gz'\n",
    "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
    "OUTPUT_NODES = ['num_detections', 'detection_boxes', 'detection_scores','detection_classes']\n",
    "\n",
    "# List of the strings that is used to add correct label for each box.\n",
    "PATH_TO_LABELS = os.path.join('data', 'mscoco_label_map.pbtxt')\n",
    "print(PATH_TO_LABELS)\n",
    "\n",
    "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
    "PATH_TO_FROZEN_GRAPH = MODEL_NAME + '/frozen_inference_graph.pb'\n",
    "print(PATH_TO_FROZEN_GRAPH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2) faster_rcnn_resnet101_kitti_2018_01_28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# What model to download.\n",
    "MODEL_NAME = './models/faster_rcnn_resnet101_kitti_2018_01_28'\n",
    "MODEL_FILE = MODEL_NAME + '.tar.gz'\n",
    "OUTPUT_NODES = ['num_detections', 'detection_boxes', 'detection_scores','detection_classes']\n",
    "\n",
    "# List of the strings that is used to add correct label for each box.\n",
    "PATH_TO_LABELS = os.path.join('data', 'kitti_label_map.pbtxt')\n",
    "print(PATH_TO_LABELS)\n",
    "\n",
    "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
    "PATH_TO_FROZEN_GRAPH = MODEL_NAME + '/frozen_inference_graph.pb'\n",
    "print(PATH_TO_FROZEN_GRAPH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3) aadc2018_frcnn_res101_200k_kitti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What model to download.\n",
    "MODEL_NAME = './models/aadc2018_frcnn_res101_200k_kitti'\n",
    "OUTPUT_NODES = ['num_detections', 'detection_boxes', 'detection_scores','detection_classes', 'detection_masks']\n",
    "\n",
    "# List of the strings that is used to add correct label for each box.\n",
    "PATH_TO_LABELS = MODEL_NAME + \"/aadc_labels_2018_without_middlelane.pbtxt\"\n",
    "print(PATH_TO_LABELS)\n",
    "\n",
    "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
    "PATH_TO_FROZEN_GRAPH = MODEL_NAME + '/aadc2018_frcnn_res101_200k_kitti.pb'\n",
    "print(PATH_TO_FROZEN_GRAPH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Model\n",
    "from Internet\n",
    "\n",
    "By default we use an \"SSD with Mobilenet\" model here. See the [detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) for a list of other models that can be run out-of-the-box with varying speeds and accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opener = urllib.request.URLopener()\n",
    "opener.retrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
    "tar_file = tarfile.open(MODEL_FILE)\n",
    "for file in tar_file.getmembers():\n",
    "    file_name = os.path.basename(file.name)\n",
    "    if 'frozen_inference_graph.pb' in file_name:\n",
    "        tar_file.extract(file, os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading label map\n",
    "Label maps map indices to category names, so that when our convolution network predicts `5`, we know that this corresponds to `airplane`.  Here we use internal utility functions, but anything that returns a dictionary mapping integers to appropriate string labels would be fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Function to read \".pb\" model (TensorRT model is stored in \".pb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (can be used to read frozen model or TensorRT model)\n",
    "def read_pb_graph(model):\n",
    "    with gfile.FastGFile(model,'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "    return graph_def"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Function to load image into numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(image):\n",
    "    (im_width, im_height) = image.size\n",
    "    return np.array(image.getdata()).reshape(\n",
    "        (im_height, im_width, 3)).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Function of showing graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "# Helper functions for TF Graph visualization\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = bytes(\"<stripped %d bytes>\"%size, 'utf-8')\n",
    "    return strip_def\n",
    "  \n",
    "def show_graph(graph_def=None, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    \n",
    "    # If no input graph is given, get the default graph\n",
    "    if graph_def is None:\n",
    "        graph_def = tf.get_default_graph().as_graph_def()\n",
    "        \n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "  \n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:800px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Showing image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    import cv2\n",
    "    import IPython\n",
    "    _,ret = cv2.imencode('.jpg',img)\n",
    "    i = IPython.display.Image(data=ret)\n",
    "    IPython.display.display(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images variables\n",
    "There are 2 images sources. Select one of them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) demo images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PATH_TO_TEST_IMAGES_DIR = './test_images2'\n",
    "TEST_IMAGE_PATHS = [ os.path.join(PATH_TO_TEST_IMAGES_DIR, 'image{}.jpg'.format(i)) for i in range(1, 3) ]\n",
    "pprint.pprint(TEST_IMAGE_PATHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Images for aadc2018_frcnn_res101_200k_kitti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_TEST_IMAGES_DIR = './models/aadc2018_frcnn_res101_200k_kitti/test_images_20181027'\n",
    "TEST_IMAGE_FILENAME = [ file for file in os.listdir(PATH_TO_TEST_IMAGES_DIR) if file.endswith(('jpg', 'png'))  ]\n",
    "print(TEST_IMAGE_FILENAME)\n",
    "\n",
    "FINAL_CONFIG_FILE = PATH_TO_TEST_IMAGES_DIR + \"/final_config.json\"\n",
    "print(FINAL_CONFIG_FILE)\n",
    "\n",
    "import json\n",
    "\n",
    "with open(FINAL_CONFIG_FILE) as json_file:\n",
    "    final_config = json.load(json_file)\n",
    "    print(len(final_config))\n",
    "\n",
    "TEST_IMAGES = []\n",
    "for filename in TEST_IMAGE_FILENAME:\n",
    "    \n",
    "    TEST_IMAGES.append({\n",
    "        \"filename\" : filename,\n",
    "        \"path\" : os.path.join(PATH_TO_TEST_IMAGES_DIR, filename),\n",
    "        \"annotations\" : [image['annotations'] for image in final_config if image['filename']==filename][0]\n",
    "    })\n",
    "    \n",
    "pprint.pprint(TEST_IMAGES[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Inference (Detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a (frozen) Tensorflow model into memory.\n",
    ".pb file is in the folder named MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.Session() as sess:\n",
    "        print('Read frozen model')\n",
    "        tf_graph = read_pb_graph(PATH_TO_FROZEN_GRAPH)\n",
    "#         show_graph(graph_def=tf_graph)\n",
    "        tf.import_graph_def(tf_graph, name='')\n",
    "    \n",
    "        # write to tensorboard (check tensorboard for each op names)\n",
    "#         writer = tf.summary.FileWriter('./logs/'+sub_dir)\n",
    "#         writer.add_graph(sess.graph)\n",
    "#         writer.flush()\n",
    "#         writer.close()\n",
    "#         print(\"\\nWrite logs {} success\\n\".format(sub_dir))\n",
    "        \n",
    "        frozen_graph = tf.graph_util.convert_variables_to_constants(\n",
    "            sess, # session\n",
    "            tf.get_default_graph().as_graph_def(),# graph+weight from the session\n",
    "            output_node_names=OUTPUT_NODES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to inference single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUTS = ['num_detections', 'detection_boxes', 'detection_scores','detection_classes', 'detection_masks']\n",
    "\n",
    "def run_inference_for_single_image(image, graph):\n",
    "    with graph.as_default():\n",
    "        with tf.Session() as sess:\n",
    "            # Get handles to input and output tensors\n",
    "            ops = tf.get_default_graph().get_operations()\n",
    "            all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
    "            tensor_dict = {}\n",
    "            for key in OUTPUTS:\n",
    "                tensor_name = key + ':0'\n",
    "                if tensor_name in all_tensor_names:\n",
    "                    tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(tensor_name)\n",
    "#             print('tensor dict')\n",
    "#             pprint.pprint(tensor_dict)\n",
    "            \n",
    "            if 'detection_masks' in tensor_dict:\n",
    "                # The following processing is only for single image\n",
    "                detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
    "                detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
    "                # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
    "                real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
    "                detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
    "                detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
    "                detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "                    detection_masks, detection_boxes, image.shape[1], image.shape[2])\n",
    "                detection_masks_reframed = tf.cast(\n",
    "                tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
    "                # Follow the convention by adding back the batch dimension\n",
    "                tensor_dict['detection_masks'] = tf.expand_dims(\n",
    "                    detection_masks_reframed, 0)\n",
    "            image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "            # Run inference\n",
    "            print(\"Run Inference\")\n",
    "            print(\"image size:\",image.shape)\n",
    "            output_dict = sess.run(tensor_dict, feed_dict={image_tensor: image}) ## Boom here\n",
    "            print(\"Finish Inference\")\n",
    "            \n",
    "            # all outputs are float32 numpy arrays, so convert types as appropriate\n",
    "            output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
    "            output_dict['detection_classes'] = output_dict['detection_classes'][0].astype(np.int64)\n",
    "            output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
    "            output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
    "            if 'detection_masks' in output_dict:\n",
    "                output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to inference all image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_inferencing(graph):  \n",
    "\n",
    "    total_time = 0\n",
    "    correct_annotations = 0\n",
    "\n",
    "    for idx, test_image in enumerate(TEST_IMAGES):\n",
    "        image_path =test_image['path']\n",
    "        image = PILImage.open(image_path)\n",
    "        image_np = load_image_into_numpy_array(image)\n",
    "    #     print(image_np.shape)\n",
    "        # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "        image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "\n",
    "        # Actual detection.\n",
    "        t1 = time.time()\n",
    "        print(\"--\")\n",
    "        output_dict = run_inference_for_single_image(image_np_expanded, graph)\n",
    "        print(\"---\")\n",
    "        t2 = time.time()\n",
    "        delta_time = t2 - t1\n",
    "        total_time += delta_time\n",
    "\n",
    "    #     print(output_dict)\n",
    "\n",
    "        # Visualization of the results of a detection.\n",
    "        image_np, box_num = vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "            image_np,\n",
    "            output_dict['detection_boxes'],\n",
    "            output_dict['detection_classes'],\n",
    "            output_dict['detection_scores'],\n",
    "            category_index,\n",
    "            instance_masks=output_dict.get('detection_masks'),\n",
    "            use_normalized_coordinates=True,\n",
    "            line_thickness=8)\n",
    "        # Saving image\n",
    "        image_new_path = image_path.replace('test_images_20181027','detected_images')\n",
    "        # for demo images\n",
    "        if image_new_path == image_path:\n",
    "            image_new_path = \"./test_images2/%d.jpg\"%idx\n",
    "        im = PILImage.fromarray(image_np, mode=\"RGB\")\n",
    "        im.save(image_new_path)\n",
    "#         imshow(image_np)\n",
    "\n",
    "        # Checking numbers of annotations\n",
    "        if box_num == len(test_image['annotations']):\n",
    "            correct_annotations += 1\n",
    "        \n",
    "        print(idx+1,'- Saving images:', test_image['filename'], \",size:\",image_np.shape, \"used time:\", delta_time, \"annotations check:\", box_num == len(test_image['annotations']))\n",
    "\n",
    "        if idx==10:\n",
    "            break\n",
    "    \n",
    "    return total_time, correct_annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "total_time, correct_annotations = main_inferencing(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = correct_annotations / TEST_IMAGES * 100\n",
    "print(\"Total time:\",total_time)\n",
    "print(\"Correct annotations:\",correct_annotations)\n",
    "print(\"Accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show deteced images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DETECTED_IMAGES_DIR = './test_images2'\n",
    "DETECTED_IMAGE_PATHS = [ os.path.join(PATH_TO_DETECTED_IMAGES_DIR, file) for file in os.listdir(PATH_TO_DETECTED_IMAGES_DIR) if file.endswith(\"jpg\") ]\n",
    "print(DETECTED_IMAGE_PATHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (12, 8)\n",
    "for idx, image_path in enumerate(DETECTED_IMAGE_PATHS):\n",
    "    image = PILImage.open(image_path)\n",
    "    plt.figure(figsize=IMAGE_SIZE)\n",
    "    plt.imshow(np.asarray(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Or...\n",
    "IMAGE_SIZE = (12, 8)\n",
    "plt.figure(figsize=IMAGE_SIZE)\n",
    "COLS = 1\n",
    "ROWS = len(DETECTED_IMAGE_PATHS) / COLS+1\n",
    "\n",
    "for idx, image_path in enumerate(DETECTED_IMAGE_PATHS):\n",
    "    image = PILImage.open(image_path)\n",
    "#     image.show()\n",
    "    plt.subplot(ROWS, COLS, idx+1 )\n",
    "    plt.imshow(np.asarray(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interferencing With TensorRT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize to TensorRT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# outputs = [out.op.name for out in model.outputs] # name of output layer\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# convert (optimize) frozen model to TensorRT model\n",
    "trt_graph = trt.create_inference_graph(\n",
    "    input_graph_def=frozen_graph,# frozen model\n",
    "    outputs=OUTPUT_NODES,\n",
    "    max_batch_size=BATCH_SIZE,# specify your max batch size\n",
    "    max_workspace_size_bytes=2*(10**9),# specify the max workspace\n",
    "    precision_mode=\"FP32\") # precision, can be \"FP32\" (32 floating point precision) or \"FP16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the TensorRT model to be used later for inference\n",
    "pb_trt_filename = \"trt_frozen_graph-\"+sub_dir+\".pb\"\n",
    "# PATH_TO_TRT_FROZEN_GRAPH = MODEL_NAME+ \"/\" + pb_trt_filename\n",
    "PATH_TO_TRT_FROZEN_GRAPH = MODEL_NAME+\"/trt_frozen_graph-2019_07_18-17_23_40.pb\"\n",
    "print(PATH_TO_TRT_FROZEN_GRAPH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gfile.FastGFile(PATH_TO_TRT_FROZEN_GRAPH, 'wb') as f:\n",
    "    f.write(trt_graph.SerializeToString())\n",
    "print(pb_trt_filename + \" is successfully stored!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load TensorRT graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trt_detection_graph = tf.Graph()\n",
    "with trt_detection_graph.as_default():\n",
    "    with tf.Session() as sess:\n",
    "        print('Read TensorRT frozen model')\n",
    "        trt_graph = read_pb_graph(PATH_TO_TRT_FROZEN_GRAPH)\n",
    "#         show_graph(graph_def=tf_graph)\n",
    "        tf.import_graph_def(trt_graph, name='')\n",
    "#         print([node.name for node in trt_graph.node])\n",
    "        \n",
    "        # write to tensorboard (check tensorboard for each op names)\n",
    "#         writer = tf.summary.FileWriter('./trt_logs/'+sub_dir)\n",
    "#         writer.add_graph(sess.graph)\n",
    "#         writer.flush()\n",
    "#         writer.close()\n",
    "#         print(\"\\nWrite trt_logs success\")\n",
    "\n",
    "# print([node.name for node in trt_detection_graph.get_operations()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trt_total_time, trt_correct_annotations = main_inferencing(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_acc = trt_correct_annotations / TEST_IMAGES * 100\n",
    "print(\"TensorRT Total time:\",trt_total_time)\n",
    "print(\"TensorRT Correct annotations:\",trt_correct_annotations)\n",
    "print(\"TensorRT Accuracy:\", trt_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Detection time: \", total_time)\n",
    "print(\"With TensorRT Detection time: \", trt_total_time)\n",
    "print(\"Different time (TensorRT faster):\", total_time - trt_total_time)\n",
    "print(\"TensorRT improvement:\", total_time/trt_total_time, \"times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
