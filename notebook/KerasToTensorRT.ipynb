{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Magma Keras to TensorRT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential,load_model, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from os.path import isfile, exists, isdir, join\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Jetson TX2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "K.set_session(tf.Session(config=config))\n",
    "\n",
    "tf.keras.backend.set_learning_phase(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_MODEL = 'MagmaCnnClassifier.hdf5'\n",
    "TARGET_SHAPE = 8\n",
    "DATA_SHAPE = (100,100,3)\n",
    "OPTIMIZER = Adam()\n",
    "BATCH_SIZE = 256\n",
    "EPOCH = 2\n",
    "\n",
    "SAVED_MODEL_DIR = './saved_model/'\n",
    "MODEL_NAME = CNN_MODEL\n",
    "\n",
    "DATA_SHAPE = (100,100,3)\n",
    "TRAIN_DIR = \"./data/train\"\n",
    "TEST_DIR = \"./data/test\"\n",
    "RESULT_PREDICTION_CALLBACK = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "### create predict_dir by move random pictures from test_dir  \n",
    "#### remove and copy test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove if exist test_dir success\n",
      "remove if exist predict_dir success\n",
      "copy test to test_dir success\n"
     ]
    }
   ],
   "source": [
    "TEST_COPY_DIR = './data/test_dir'\n",
    "PREDICT_DIR = './data/predict_dir'\n",
    "\n",
    "# delete folder if exist\n",
    "if exists(TEST_COPY_DIR) and isdir(TEST_COPY_DIR):\n",
    "    shutil.rmtree(TEST_COPY_DIR)\n",
    "print('remove if exist test_dir success')\n",
    "\n",
    "if exists(PREDICT_DIR) and isdir(PREDICT_DIR):\n",
    "    shutil.rmtree(PREDICT_DIR)\n",
    "print('remove if exist predict_dir success')\n",
    "\n",
    "#copy test as test_dir, there are result as list of copy files\n",
    "# from distutils.dir_util import copy_tree\n",
    "# copy_tree('./data/test','./data/test_dir')\n",
    "from subprocess import call\n",
    "call(['cp','-a', TEST_DIR, TEST_COPY_DIR])\n",
    "print('copy test to test_dir success')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create predict_dir and random moving images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copy - category: 0 ['7470.png']\n",
      "copy - category: 1 ['7860.png']\n",
      "copy - category: 2 ['7886.png']\n",
      "copy - category: 3 ['9877.png']\n",
      "copy - category: 4 ['4808.png']\n",
      "copy - category: 5 ['527.png']\n",
      "copy - category: 6 ['1231.png']\n",
      "copy - category: 7 ['2538.png']\n"
     ]
    }
   ],
   "source": [
    "#random select images\n",
    "CATEGORIES = ['0','1','2','3','4','5','6','7']\n",
    "IMAGES_PER_FOLDER = 1\n",
    "\n",
    "import random\n",
    "for category in CATEGORIES:\n",
    "    \n",
    "    path_ct = join(TEST_COPY_DIR,category)\n",
    "    path_pd = join(PREDICT_DIR, category)\n",
    "    \n",
    "    if not exists(path_pd):\n",
    "        os.makedirs(path_pd)\n",
    "    \n",
    "    image_list = os.listdir(path_ct)\n",
    "    random.shuffle(image_list)\n",
    "    \n",
    "    for img in image_list[:IMAGES_PER_FOLDER]:\n",
    "        path_src = join(path_ct,img)\n",
    "        path_des = join(path_pd,img)\n",
    "        shutil.move(path_src, path_des)\n",
    "    \n",
    "    print('copy - category:',category, image_list[:IMAGES_PER_FOLDER])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WorkFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras to TensorRT\n",
    "![alt text](pictures/Keras_to_TensorRT.png)\n",
    "\n",
    "### Tensorflow to TensorRT\n",
    "![alt text](pictures/tf-trt_workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a0. Convert Keras to Tensorflow model\n",
    "### 1) Build Keras model (Declare + Train)\n",
    "#### 1.1) Declare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 98, 98, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 49, 49, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 47, 47, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 23, 23, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 21, 21, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 10, 10, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 8)                 4104      \n",
      "=================================================================\n",
      "Total params: 1,294,024\n",
      "Trainable params: 1,294,024\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=DATA_SHAPE),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(filters=128, kernel_size=(3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(filters=128, kernel_size=(3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dropout(rate=0.5),\n",
    "    Dense(units=512, activation='relu'),\n",
    "    Dense(units=TARGET_SHAPE, activation='softmax'),\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                    optimizer=OPTIMIZER,\n",
    "                    metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2) create generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4917 images belonging to 8 classes.\n",
      "Found 1222 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1 / 255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1 / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory = TRAIN_DIR ,\n",
    "    target_size = DATA_SHAPE[:2],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    directory = TEST_COPY_DIR, # use test data that be splited for prediction\n",
    "    target_size = DATA_SHAPE[:2],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3) Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "19/19 [==============================] - 51s 3s/step - loss: 1.6606 - categorical_accuracy: 0.4002 - val_loss: 1.1081 - val_categorical_accuracy: 0.6055\n",
      "Epoch 2/2\n",
      "19/19 [==============================] - 36s 2s/step - loss: 1.1837 - categorical_accuracy: 0.5959 - val_loss: 0.7582 - val_categorical_accuracy: 0.7619\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f08171ac8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_dir = time.strftime(\"%Y_%m_%d-%H_%M_%S\", time.localtime())\n",
    "tb_cb = TensorBoard(log_dir='./logs/'+sub_dir,\n",
    "                    #histogram_freq=EPOCHS,\n",
    "                    batch_size=BATCH_SIZE, \n",
    "                    write_grads=True,\n",
    "                    write_images=True,\n",
    "                    #update_freq='batch',\n",
    "                    )\n",
    "callbacks = [tb_cb]\n",
    "\n",
    "# train_generator, val_generator declare Data preprocessing\n",
    "\n",
    "model.fit_generator(generator=train_generator,\n",
    "                              steps_per_epoch=train_generator.n//BATCH_SIZE,\n",
    "                                epochs=EPOCH,\n",
    "                                validation_data=val_generator,\n",
    "                              validation_steps=val_generator.n//BATCH_SIZE,\n",
    "                                callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@pipidog/how-to-convert-your-keras-models-to-tensorflow-e471400b886a\n",
    "# 1) build keras model + train\n",
    "x = np.vstack((np.random.rand(1000,10),-np.random.rand(1000,10)))\n",
    "y = np.vstack((np.ones((1000,1)),np.zeros((1000,1))))\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units = 32, input_shape=(10,), activation='relu'))\n",
    "model.add(Dense(units = 16, activation='relu'))\n",
    "model.add(Dense(units = 1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='Adam',metrics=['binary_accuracy'])\n",
    "model.fit(x=x,y=y,epochs=2,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --save model .hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.save(SAVED_MODEL_DIR+CNN_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --load model .hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if isfile(SAVED_MODEL_DIR + MODEL_NAME):\n",
    "    model = load_model(filepath= SAVED_MODEL_DIR + MODEL_NAME)\n",
    "    model.summary()\n",
    "else:\n",
    "    raise Exception(\"--MODEL COULD NOT LOADED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Save the model to Protocol Buffers Format (.pb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pb_filename = 'model.pb'\n",
    "wkdir = './saved_model/'\n",
    "\n",
    "def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\n",
    "    \"\"\"\n",
    "    Freezes the state of a session into a pruned computation graph.\n",
    "    Creates a new computation graph where variable nodes are replaced by\n",
    "    constants taking their current value in the session. The new graph will be\n",
    "    pruned so subgraphs that are not necessary to compute the requested\n",
    "    outputs are removed.\n",
    "    @param session The TensorFlow session to be frozen.\n",
    "    @param keep_var_names A list of variable names that should not be frozen,\n",
    "                          or None to freeze all the variables in the graph.\n",
    "    @param output_names Names of the relevant graph outputs.\n",
    "    @param clear_devices Remove the device directives from the graph for better portability.\n",
    "    @return The frozen graph definition.\n",
    "    \"\"\"\n",
    "    from tensorflow.python.framework.graph_util import convert_variables_to_constants\n",
    "    graph = session.graph\n",
    "    with graph.as_default():\n",
    "        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n",
    "        output_names = output_names or []\n",
    "        output_names += [v.op.name for v in tf.global_variables()]\n",
    "        input_graph_def = graph.as_graph_def()\n",
    "        if clear_devices:\n",
    "            for node in input_graph_def.node:\n",
    "                node.device = \"\"\n",
    "        frozen_graph = convert_variables_to_constants(session, input_graph_def, output_names, freeze_var_names)\n",
    "        return frozen_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1) save keras model as tf pb file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 104 variables.\n",
      "INFO:tensorflow:Converted 104 variables to const ops.\n",
      "Frozen model is successfully stored!\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from tensorflow.python.platform import gfile\n",
    "\n",
    "K.set_session(tf.Session(graph=model.output.graph)) \n",
    "init = K.tf.global_variables_initializer() \n",
    "K.get_session().run(init)\n",
    "\n",
    "frozen_graph = freeze_session(K.get_session(), output_names=[out.op.name for out in model.outputs])\n",
    "\n",
    "# tf.trian.write_graph(frozen_graph, wkdir, pb_filename, as_text=False)\n",
    "\n",
    "#write the TensorRT model to be used later for inference\n",
    "with gfile.FastGFile(\"./saved_model/Magma_frozen_model.pb\", 'wb') as f:\n",
    "    f.write(frozen_graph.SerializeToString())\n",
    "print(\"Frozen model is successfully stored!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a pb file by tensorflow & Inference using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##load & inference the model ==================\n",
    "\n",
    "from tensorflow.python.platform import gfile\n",
    "with tf.Session() as sess:\n",
    "    # load model from pb file\n",
    "    with gfile.FastGFile(wkdir+'/'+pb_filename,'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "        sess.graph.as_default()\n",
    "        g_in = tf.import_graph_def(graph_def)\n",
    "    # write to tensorboard (check tensorboard for each op names)\n",
    "    writer = tf.summary.FileWriter(wkdir+'/log/')\n",
    "    writer.add_graph(sess.graph)\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "    # print all operation names \n",
    "    print('\\n===== ouptut operation names =====\\n')\n",
    "    for op in sess.graph.get_operations():\n",
    "        print(op)\n",
    "    # inference by the model (op name must comes with :0 to specify the index of its output)\n",
    "    tensor_output = sess.graph.get_tensor_by_name('import/dense_3/Sigmoid:0')\n",
    "    tensor_input = sess.graph.get_tensor_by_name('import/dense_1_input:0')\n",
    "    predictions = sess.run(tensor_output, {tensor_input: x})\n",
    "    print('\\n===== output predicted results =====\\n')\n",
    "    print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ======save the model to Tensorflow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model(SAVED_MODEL_DIR + MODEL_NAME)\n",
    "\n",
    "init_g = tf.global_variables_initializer()\n",
    "init_l = tf.local_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_g)\n",
    "    sess.run(init_l)\n",
    "    saver = tf.train.Saver()\n",
    "    sess = tf.keras.backend.get_session()\n",
    "    save_path = saver.save(sess, SAVED_MODEL_DIR)\n",
    "\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "model = VGG16(weights='imagenet')\n",
    "\n",
    "\n",
    "print(\"Keras model is successfully converted to TF model in \"+SAVED_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calulate Time, mAP, ACC of Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Process in TensorRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calculate Time, mAP, ACC of converted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
