{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Magma Keras to TensorRT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential,load_model, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.tensorrt as trt\n",
    "from tensorflow.python.platform import gfile\n",
    "\n",
    "\n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from os.path import isfile, exists, isdir, join\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Jetson TX2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "K.set_session(tf.Session(config=config))\n",
    "\n",
    "tf.keras.backend.set_learning_phase(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_MODEL = 'MagmaCnnClassifier.hdf5'\n",
    "TARGET_SHAPE = 8\n",
    "DATA_SHAPE = (100,100,3)\n",
    "OPTIMIZER = Adam()\n",
    "BATCH_SIZE = 256\n",
    "EPOCH = 2\n",
    "\n",
    "SAVED_MODEL_DIR = './saved_model/'\n",
    "MODEL_NAME = CNN_MODEL\n",
    "\n",
    "DATA_SHAPE = (100,100,3)\n",
    "TRAIN_DIR = \"./data/train\"\n",
    "TEST_DIR = \"./data/test\"\n",
    "RESULT_PREDICTION_CALLBACK = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "### create predict_dir by move random pictures from test_dir  \n",
    "#### remove and copy test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_COPY_DIR = './data/test_dir'\n",
    "PREDICT_DIR = './data/predict_dir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete folder if exist\n",
    "if exists(TEST_COPY_DIR) and isdir(TEST_COPY_DIR):\n",
    "    shutil.rmtree(TEST_COPY_DIR)\n",
    "print('remove if exist test_dir success')\n",
    "\n",
    "if exists(PREDICT_DIR) and isdir(PREDICT_DIR):\n",
    "    shutil.rmtree(PREDICT_DIR)\n",
    "print('remove if exist predict_dir success')\n",
    "\n",
    "#copy test as test_dir, there are result as list of copy files\n",
    "# from distutils.dir_util import copy_tree\n",
    "# copy_tree('./data/test','./data/test_dir')\n",
    "from subprocess import call\n",
    "call(['cp','-a', TEST_DIR, TEST_COPY_DIR])\n",
    "print('copy test to test_dir success')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create predict_dir and random moving images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random select images\n",
    "CATEGORIES = ['0','1','2','3','4','5','6','7']\n",
    "IMAGES_PER_FOLDER = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "for category in CATEGORIES:\n",
    "    \n",
    "    path_ct = join(TEST_COPY_DIR,category)\n",
    "    path_pd = join(PREDICT_DIR, category)\n",
    "    \n",
    "    if not exists(path_pd):\n",
    "        os.makedirs(path_pd)\n",
    "    \n",
    "    image_list = os.listdir(path_ct)\n",
    "    random.shuffle(image_list)\n",
    "    \n",
    "    for img in image_list[:IMAGES_PER_FOLDER]:\n",
    "        path_src = join(path_ct,img)\n",
    "        path_des = join(path_pd,img)\n",
    "        shutil.move(path_src, path_des)\n",
    "    \n",
    "    print('copy - category:',category, image_list[:IMAGES_PER_FOLDER])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WorkFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras to TensorRT\n",
    "![alt text](pictures/Keras_to_TensorRT.png)\n",
    "\n",
    "### Tensorflow to TensorRT\n",
    "![alt text](pictures/tf-trt_workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a0. Convert Keras to Tensorflow model and a) Read input Tensorflow model\n",
    "##### don't need to read input Tensorflow because i don't save .mega tensorflow file from keras\n",
    "### Build Keras model (Declare + Train)\n",
    "#### a.1) Declare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    # Layer 1\n",
    "    Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=DATA_SHAPE, name='input_tensor'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    # Layer 2\n",
    "    Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    # Layer 3\n",
    "    Conv2D(filters=128, kernel_size=(3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    # Layer 4\n",
    "    Conv2D(filters=128, kernel_size=(3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    # flattening the model for fully connected layer\n",
    "    Flatten(),\n",
    "    Dropout(rate=0.5),\n",
    "    # fully connected layer\n",
    "    Dense(units=512, activation='relu'),\n",
    "    # output layer\n",
    "    Dense(units=TARGET_SHAPE, activation='softmax', name='output_tensor'),\n",
    "])\n",
    "\n",
    "# Compilile the network\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                    optimizer=OPTIMIZER,\n",
    "                    metrics=['categorical_accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a.2) create generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1 / 255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1 / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory = TRAIN_DIR ,\n",
    "    target_size = DATA_SHAPE[:2],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    directory = TEST_COPY_DIR, # use test data that be splited for prediction\n",
    "    target_size = DATA_SHAPE[:2],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a.3) Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_dir = time.strftime(\"%Y_%m_%d-%H_%M_%S\", time.localtime())\n",
    "tb_cb = TensorBoard(log_dir='./logs/'+sub_dir,\n",
    "                    #histogram_freq=EPOCHS,\n",
    "                    batch_size=BATCH_SIZE, \n",
    "                    write_grads=True,\n",
    "                    write_images=True,\n",
    "                    #update_freq='batch',\n",
    "                    )\n",
    "callbacks = [tb_cb]\n",
    "\n",
    "# train_generator, val_generator declare Data preprocessing\n",
    "\n",
    "model.fit_generator(generator=train_generator,\n",
    "                              steps_per_epoch=train_generator.n//BATCH_SIZE,\n",
    "                                epochs=EPOCH,\n",
    "                                validation_data=val_generator,\n",
    "                              validation_steps=val_generator.n//BATCH_SIZE,\n",
    "                                callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# name of all nodes\n",
    "[n.name for n in tf.get_default_graph().as_graph_def().node]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ----------------------------------------example model from medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 10)\n",
      "(2000, 1)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 32)                352       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 897\n",
      "Trainable params: 897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/2\n",
      "1600/1600 [==============================] - 2s 2ms/step - loss: 0.6743 - binary_accuracy: 0.5819 - val_loss: 0.2621 - val_binary_accuracy: 1.0000\n",
      "Epoch 2/2\n",
      "1600/1600 [==============================] - 1s 643us/step - loss: 0.2531 - binary_accuracy: 1.0000 - val_loss: 0.0622 - val_binary_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2cc37ef0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://medium.com/@pipidog/how-to-convert-your-keras-models-to-tensorflow-e471400b886a\n",
    "# 1) build keras model + train\n",
    "x = np.vstack((np.random.rand(1000,10),-np.random.rand(1000,10)))\n",
    "y = np.vstack((np.ones((1000,1)),np.zeros((1000,1))))\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units = 32, input_shape=(10,), activation='relu'))\n",
    "model.add(Dense(units = 16, activation='relu'))\n",
    "model.add(Dense(units = 1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='Adam',metrics=['binary_accuracy'])\n",
    "model.fit(x=x,y=y,epochs=2,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --save model .hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.save(SAVED_MODEL_DIR+CNN_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --load model .hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if isfile(SAVED_MODEL_DIR + MODEL_NAME):\n",
    "    model = load_model(filepath= SAVED_MODEL_DIR + MODEL_NAME)\n",
    "    model.summary()\n",
    "else:\n",
    "    raise Exception(\"--MODEL COULD NOT LOADED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras to TensorRT\n",
    "![alt text](pictures/Keras_to_TensorRT.png)\n",
    "## b) Convert to Frozen model .pb\n",
    "#### b.1) declare function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\n",
    "    \"\"\"\n",
    "    Freezes the state of a session into a pruned computation graph.\n",
    "    Creates a new computation graph where variable nodes are replaced by\n",
    "    constants taking their current value in the session. The new graph will be\n",
    "    pruned so subgraphs that are not necessary to compute the requested\n",
    "    outputs are removed.\n",
    "    @param session The TensorFlow session to be frozen.\n",
    "    @param keep_var_names A list of variable names that should not be frozen,\n",
    "                          or None to freeze all the variables in the graph.\n",
    "    @param output_names Names of the relevant graph outputs.\n",
    "    @param clear_devices Remove the device directives from the graph for better portability.\n",
    "    @return The frozen graph definition.\n",
    "    \"\"\"\n",
    "    from tensorflow.python.framework.graph_util import convert_variables_to_constants\n",
    "    graph = session.graph\n",
    "    with graph.as_default():\n",
    "        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n",
    "        output_names = output_names or []\n",
    "        output_names += [v.op.name for v in tf.global_variables()]\n",
    "        input_graph_def = graph.as_graph_def()\n",
    "        if clear_devices:\n",
    "            for node in input_graph_def.node:\n",
    "                node.device = \"\"\n",
    "        frozen_graph = convert_variables_to_constants(session, input_graph_def, output_names, freeze_var_names)\n",
    "        return frozen_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b.2) Save the model to Protocol Buffers Format (.pb) as tf pb file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "K.set_learning_phase(0)\n",
    "\n",
    "model = load_model(SAVED_MODEL_DIR + MODEL_NAME)\n",
    "\n",
    "MODEL_PATH = \"./saved_model/test/\"\n",
    "saver = tf.train.Saver()\n",
    "K.get_session().run(K.tf.global_variables_initializer())\n",
    "sess = K.get_session()\n",
    "save_path = saver.save(sess, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.output.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dense_3/Sigmoid']\n",
      "dense_3/Sigmoid\n",
      "['dense_1_input']\n",
      "dense_1_input\n",
      "INFO:tensorflow:Froze 34 variables.\n",
      "INFO:tensorflow:Converted 34 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from tensorflow.python.platform import gfile\n",
    "\n",
    "OUTPUTS = [out.op.name for out in model.outputs]\n",
    "print(OUTPUTS)\n",
    "print(OUTPUTS[0])\n",
    "INPUTS = [inp.op.name for inp in model.inputs]\n",
    "print(INPUTS)\n",
    "print(INPUTS[0])\n",
    "\n",
    "# model is used here \n",
    "# K.set_session(tf.Session(graph=model.output.graph)) \n",
    "init = K.tf.global_variables_initializer() \n",
    "K.get_session().run(init)\n",
    "\n",
    "frozen_graph = freeze_session(K.get_session(), output_names=OUTPUTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frozen model is successfully stored!\n"
     ]
    }
   ],
   "source": [
    "with gfile.FastGFile(\"./saved_model/sampleConvertFrozen.pb\", 'wb') as f:\n",
    "    f.write(frozen_graph.SerializeToString())\n",
    "print(\"Frozen model is successfully stored!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_now = time.strftime(\"%d-%m-%Y_%H-%M-%S\", time.localtime())\n",
    "filename = \"Magma_frozen_model_\"+time_now+\".pb\"\n",
    "\n",
    "#write the TensorRT model to be used later for inference\n",
    "with gfile.FastGFile(\"./saved_model/\"+filename, 'wb') as f:\n",
    "    f.write(frozen_graph.SerializeToString())\n",
    "print(\"Frozen model is successfully stored!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inp in model.inputs:\n",
    "    print(inp.op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for out in model.outputs:\n",
    "    print(out.op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ---Load a pb file by tensorflow & Inference using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##load & inference the model ==================\n",
    "from tensorflow.python.platform import gfile\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # load model from pb file\n",
    "    with gfile.FastGFile(SAVED_MODEL_DIR+pb_filename,'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "        sess.graph.as_default()\n",
    "        g_in = tf.import_graph_def(graph_def)\n",
    "    \n",
    "    # write to tensorboard (check tensorboard for each op names)\n",
    "    writer = tf.summary.FileWriter(SAVED_MODEL_DIR+'forzen_log/')\n",
    "    writer.add_graph(sess.graph)\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "    \n",
    "    # print all operation names \n",
    "    print('\\n===== ouptut operation names =====\\n')\n",
    "    for op in sess.graph.get_operations():\n",
    "        print(op)\n",
    "    \n",
    "    # inference by the model (op name must comes with :0 to specify the index of its output)\n",
    "    tensor_output = sess.graph.get_tensor_by_name('import/output_tensor/Softmax:0')\n",
    "    tensor_input = sess.graph.get_tensor_by_name('import/input_tensor_input:0')\n",
    "    #TODO change get tensor by name to fit to Magma\n",
    "    predictions = sess.run(tensor_output, {tensor_input: x})\n",
    "    print('\\n===== output predicted results =====\\n')\n",
    "    print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras to TensorRT\n",
    "![alt text](pictures/Keras_to_TensorRT.png)\n",
    "## c) Optimize the frozen model to TensorRT graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs = [\"dense_2/Softmax\"]\n",
    "# outputs = [\"output_tensor/Softmax\"]\n",
    "\n",
    "# convert (optimize) frozen model to TensorRT model\n",
    "trt_graph = trt.create_inference_graph(\n",
    "    input_graph_def=frozen_graph,# frozen model\n",
    "    outputs=OUTPUTS,\n",
    "    # outputs=outputs,\n",
    "    max_batch_size=2,# specify your max batch size\n",
    "    max_workspace_size_bytes=2*(10**9),# specify the max workspace\n",
    "    precision_mode=\"FP32\") # precision, can be \"FP32\" (32 floating point precision) or \"FP16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_now = time.strftime(\"%d-%m-%Y_%H-%M-%S\", time.localtime())\n",
    "tr_filename = \"TensorRT_Magma_model_\"+time_now+\".pb\"\n",
    "\n",
    "#write the TensorRT model to be used later for inference\n",
    "with gfile.FastGFile(\"./saved_model/\"+tr_filename, 'wb') as f:\n",
    "    f.write(trt_graph.SerializeToString())\n",
    "print(\"TensorRT model is successfully stored!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count how many nodes/operations before and after optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many ops of the original frozen model\n",
    "all_nodes = len([1 for n in frozen_graph.node])\n",
    "print(\"numb. of all_nodes in frozen graph:\", all_nodes)\n",
    "\n",
    "# check how many ops that is converted to TensorRT engine\n",
    "trt_engine_nodes = len([1 for n in trt_graph.node if str(n.op) == 'TRTEngineOp'])\n",
    "print(\"numb. of trt_engine_nodes in TensorRT graph:\", trt_engine_nodes)\n",
    "all_nodes = len([1 for n in trt_graph.node])\n",
    "print(\"numb. of all_nodes in TensorRT graph:\", all_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) Inference using TensorRT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d.1) Function to read \".pb\" model (TensorRT model is stored in \".pb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (can be used to read frozen model or TensorRT model)\n",
    "def read_pb_graph(model):\n",
    "    with gfile.FastGFile(model,'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "    return graph_def"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepare data for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['./data/predict_dir/0/3641.png', 0, '3641.png'], ['./data/predict_dir/1/5852.png', 1, '5852.png'], ['./data/predict_dir/2/5968.png', 2, '5968.png'], ['./data/predict_dir/3/2629.png', 3, '2629.png'], ['./data/predict_dir/4/9928.png', 4, '9928.png'], ['./data/predict_dir/5/9314.png', 5, '9314.png'], ['./data/predict_dir/6/898.png', 6, '898.png'], ['./data/predict_dir/7/2347.png', 7, '2347.png']]\n",
      "./data/predict_dir/0/3641.png\n"
     ]
    }
   ],
   "source": [
    "images = []\n",
    "for category in CATEGORIES:\n",
    "    path_pd = join(PREDICT_DIR, category)\n",
    "    class_num = CATEGORIES.index(category)\n",
    "    image_list = os.listdir(path_pd)\n",
    "    \n",
    "    for img in image_list:\n",
    "        images.append([join(path_pd,img),class_num,img])\n",
    "\n",
    "print(images)\n",
    "print(images[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 100, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "COLS = 5\n",
    "rows = len(images)//COLS+1\n",
    "\n",
    "input_img = []\n",
    "\n",
    "for idx, image in enumerate(images):\n",
    "    col = idx%COLS\n",
    "    row = idx//COLS\n",
    "    \n",
    "    img = load_img(path=image[0], color_mode='rgb', target_size=DATA_SHAPE)\n",
    "    # img = img_to_array(img)\n",
    "    img2predict = img.copy()\n",
    "    img2predict = img_to_array(img2predict)\n",
    "    # img2predict = np.expand_dims(img2predict,0)\n",
    "    img2predict /= 255\n",
    "    # int(img2predict)\n",
    "    # print(img2predict.shape)\n",
    "    input_img.append(img2predict)\n",
    "    \n",
    "input_img = np.array(input_img)\n",
    "print(input_img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d.2) Perform inference using TensorRT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read TensorRT model\n",
      "obtain the corresponding input-output tensor\n",
      "in this case, it demonstrates to perform inference for 50 times\n",
      "before Sess run\n"
     ]
    }
   ],
   "source": [
    "# variable\n",
    "# print(tr_filename)\n",
    "# TENSORRT_MODEL_PATH = './saved_model/'+tr_filename\n",
    "\n",
    "TENSORRT_MODEL_PATH = './saved_model/TensorRT_Magma_model_01-07-2019_16-54-31.pb'\n",
    "\n",
    "# INPUT_TENSOR = INPUTS + ':0'\n",
    "# OUTPUT_TENSOR = OUTPUTS + ':0'\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.50))) as sess:\n",
    "        # read TensorRT model\n",
    "        print('read TensorRT model')\n",
    "        trt_graph = read_pb_graph(TENSORRT_MODEL_PATH)\n",
    "\n",
    "        # obtain the corresponding input-output tensor\n",
    "        print('obtain the corresponding input-output tensor')\n",
    "        tf.import_graph_def(trt_graph, name='')\n",
    "        input = sess.graph.get_tensor_by_name('input_tensor_input:0')\n",
    "        output = sess.graph.get_tensor_by_name('output_tensor/Softmax:0')\n",
    "\n",
    "        # in this case, it demonstrates to perform inference for 50 times\n",
    "        print('in this case, it demonstrates to perform inference for 50 times')\n",
    "        total_time = 0; n_time_inference = 3\n",
    "        print('before Sess run')\n",
    "        out_pred = sess.run(output, feed_dict={input: input_img}) # <--------- boom here\n",
    "        print('after Sess run')\n",
    "        for i in range(n_time_inference):\n",
    "            print('in loop',i)\n",
    "            t1 = time.time()\n",
    "            out_pred = sess.run(output, feed_dict={input: input_img})\n",
    "            t2 = time.time()\n",
    "            delta_time = t2 - t1\n",
    "            total_time += delta_time\n",
    "            print(\"needed time in inference-\" + str(i) + \": \", delta_time)\n",
    "        avg_time_tensorRT = total_time / n_time_inference\n",
    "        print(\"average inference time: \", avg_time_tensorRT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d.3) Perform inference using the original tensorflow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable\n",
    "FROZEN_MODEL_PATH = './saved_model/Magma_frozen_model.pb'\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.Session() as sess:\n",
    "        # read TensorRT model\n",
    "        frozen_graph = read_pb_graph(FROZEN_MODEL_PATH)\n",
    "\n",
    "        # obtain the corresponding input-output tensor\n",
    "        tf.import_graph_def(frozen_graph, name='')\n",
    "        input = sess.graph.get_tensor_by_name('conv2d_5_input:0')\n",
    "        output = sess.graph.get_tensor_by_name('output_tensor_1/Softmax:0')\n",
    "\n",
    "        # in this case, it demonstrates to perform inference for 50 times\n",
    "        total_time = 0; n_time_inference = 50\n",
    "        out_pred = sess.run(output, feed_dict={input: input_img})\n",
    "        for i in range(n_time_inference):\n",
    "            t1 = time.time()\n",
    "            out_pred = sess.run(output, feed_dict={input: input_img})\n",
    "            t2 = time.time()\n",
    "            delta_time = t2 - t1\n",
    "            total_time += delta_time\n",
    "            print(\"needed time in inference-\" + str(i) + \": \", delta_time)\n",
    "        avg_time_original_model = total_time / n_time_inference\n",
    "        print(\"average inference time: \", avg_time_original_model)\n",
    "        print(\"TensorRT improvement compared to the original model:\", avg_time_original_model/avg_time_tensorRT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d.4) Plot the prediction result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pred in out_pred:\n",
    "#     plt.figure('img 1')\n",
    "    plt.imshow(img1, cmap='rgb')\n",
    "    plt.title('pred:' + str(np.argmax(pred)), fontsize=22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the original and optimized graphs\n",
    "Using [netron](https://lutzroeder.github.io/netron/), the web application for vitsulaize model graph by upload .pb file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calulate Time, mAP, ACC of Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Process in TensorRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calculate Time, mAP, ACC of converted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
