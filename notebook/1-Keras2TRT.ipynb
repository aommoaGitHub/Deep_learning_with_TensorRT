{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Magma Keras to TensorRT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential,load_model, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.tensorrt as trt\n",
    "from tensorflow.python.platform import gfile\n",
    "\n",
    "\n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from os.path import isfile, exists, isdir, join\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Jetson TX2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "K.set_session(tf.Session(config=config))\n",
    "\n",
    "tf.keras.backend.set_learning_phase(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_MODEL = 'MagmaCnnClassifier.hdf5'\n",
    "TARGET_SHAPE = 8\n",
    "DATA_SHAPE = (100,100,3)\n",
    "OPTIMIZER = Adam()\n",
    "BATCH_SIZE = 256\n",
    "EPOCH = 2\n",
    "\n",
    "SAVED_MODEL_DIR = './saved_model/'\n",
    "MODEL_NAME = CNN_MODEL\n",
    "\n",
    "DATA_SHAPE = (100,100,3)\n",
    "TRAIN_DIR = \"./data/train\"\n",
    "TEST_DIR = \"./data/test\"\n",
    "RESULT_PREDICTION_CALLBACK = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "### create predict_dir by move random pictures from test_dir  \n",
    "#### remove and copy test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_COPY_DIR = './data/test_dir'\n",
    "PREDICT_DIR = './data/predict_dir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete folder if exist\n",
    "if exists(TEST_COPY_DIR) and isdir(TEST_COPY_DIR):\n",
    "    shutil.rmtree(TEST_COPY_DIR)\n",
    "print('remove if exist test_dir success')\n",
    "\n",
    "if exists(PREDICT_DIR) and isdir(PREDICT_DIR):\n",
    "    shutil.rmtree(PREDICT_DIR)\n",
    "print('remove if exist predict_dir success')\n",
    "\n",
    "#copy test as test_dir, there are result as list of copy files\n",
    "# from distutils.dir_util import copy_tree\n",
    "# copy_tree('./data/test','./data/test_dir')\n",
    "from subprocess import call\n",
    "call(['cp','-a', TEST_DIR, TEST_COPY_DIR])\n",
    "print('copy test to test_dir success')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create predict_dir and random moving images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random select images\n",
    "CATEGORIES = ['0','1','2','3','4','5','6','7']\n",
    "IMAGES_PER_FOLDER = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "for category in CATEGORIES:\n",
    "    \n",
    "    path_ct = join(TEST_COPY_DIR,category)\n",
    "    path_pd = join(PREDICT_DIR, category)\n",
    "    \n",
    "    if not exists(path_pd):\n",
    "        os.makedirs(path_pd)\n",
    "    \n",
    "    image_list = os.listdir(path_ct)\n",
    "    random.shuffle(image_list)\n",
    "    \n",
    "    for img in image_list[:IMAGES_PER_FOLDER]:\n",
    "        path_src = join(path_ct,img)\n",
    "        path_des = join(path_pd,img)\n",
    "        shutil.move(path_src, path_des)\n",
    "    \n",
    "    print('copy - category:',category, image_list[:IMAGES_PER_FOLDER])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WorkFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras to TensorRT\n",
    "![alt text](pictures/Keras_to_TensorRT.png)\n",
    "\n",
    "### Tensorflow to TensorRT\n",
    "![alt text](pictures/tf-trt_workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a0. Convert Keras to Tensorflow model and a) Read input Tensorflow model\n",
    "##### don't need to read input Tensorflow because i don't save .mega tensorflow file from keras\n",
    "### Build Keras model (Declare + Train)\n",
    "#### a.1) Declare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    # Layer 1\n",
    "    Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=DATA_SHAPE, name='input_tensor'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    # Layer 2\n",
    "    Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    # Layer 3\n",
    "    Conv2D(filters=128, kernel_size=(3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    # Layer 4\n",
    "    Conv2D(filters=128, kernel_size=(3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    # flattening the model for fully connected layer\n",
    "    Flatten(),\n",
    "    Dropout(rate=0.5),\n",
    "    # fully connected layer\n",
    "    Dense(units=512, activation='relu'),\n",
    "    # output layer\n",
    "    Dense(units=TARGET_SHAPE, activation='softmax', name='output_tensor'),\n",
    "])\n",
    "\n",
    "# Compilile the network\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                    optimizer=OPTIMIZER,\n",
    "                    metrics=['categorical_accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a.2) create generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1 / 255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1 / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory = TRAIN_DIR ,\n",
    "    target_size = DATA_SHAPE[:2],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    directory = TEST_COPY_DIR, # use test data that be splited for prediction\n",
    "    target_size = DATA_SHAPE[:2],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a.3) Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_dir = time.strftime(\"%Y_%m_%d-%H_%M_%S\", time.localtime())\n",
    "tb_cb = TensorBoard(log_dir='./logs/'+sub_dir,\n",
    "                    #histogram_freq=EPOCHS,\n",
    "                    batch_size=BATCH_SIZE, \n",
    "                    write_grads=True,\n",
    "                    write_images=True,\n",
    "                    #update_freq='batch',\n",
    "                    )\n",
    "callbacks = [tb_cb]\n",
    "\n",
    "# train_generator, val_generator declare Data preprocessing\n",
    "\n",
    "model.fit_generator(generator=train_generator,\n",
    "                              steps_per_epoch=train_generator.n//BATCH_SIZE,\n",
    "                                epochs=EPOCH,\n",
    "                                validation_data=val_generator,\n",
    "                              validation_steps=val_generator.n//BATCH_SIZE,\n",
    "                                callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# name of all nodes\n",
    "[n.name for n in tf.get_default_graph().as_graph_def().node]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --save model .hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.save(SAVED_MODEL_DIR+CNN_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --load model .hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if isfile(SAVED_MODEL_DIR + MODEL_NAME):\n",
    "    model = load_model(filepath= SAVED_MODEL_DIR + MODEL_NAME)\n",
    "    model.summary()\n",
    "else:\n",
    "    raise Exception(\"--MODEL COULD NOT LOADED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras to TensorRT\n",
    "![alt text](pictures/Keras_to_TensorRT.png)\n",
    "## b) Convert to Frozen model .pb\n",
    "#### b.1) declare function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\n",
    "    \"\"\"\n",
    "    Freezes the state of a session into a pruned computation graph.\n",
    "    Creates a new computation graph where variable nodes are replaced by\n",
    "    constants taking their current value in the session. The new graph will be\n",
    "    pruned so subgraphs that are not necessary to compute the requested\n",
    "    outputs are removed.\n",
    "    @param session The TensorFlow session to be frozen.\n",
    "    @param keep_var_names A list of variable names that should not be frozen,\n",
    "                          or None to freeze all the variables in the graph.\n",
    "    @param output_names Names of the relevant graph outputs.\n",
    "    @param clear_devices Remove the device directives from the graph for better portability.\n",
    "    @return The frozen graph definition.\n",
    "    \"\"\"\n",
    "    from tensorflow.python.framework.graph_util import convert_variables_to_constants\n",
    "    graph = session.graph\n",
    "    with graph.as_default():\n",
    "        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n",
    "        output_names = output_names or []\n",
    "        output_names += [v.op.name for v in tf.global_variables()]\n",
    "        input_graph_def = graph.as_graph_def()\n",
    "        if clear_devices:\n",
    "            for node in input_graph_def.node:\n",
    "                node.device = \"\"\n",
    "        frozen_graph = convert_variables_to_constants(session, input_graph_def, output_names, freeze_var_names)\n",
    "        return frozen_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b.2) Save the model to Protocol Buffers Format (.pb) as tf pb file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inp in model.inputs:\n",
    "    print(inp.op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for out in model.outputs:\n",
    "    print(out.op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from tensorflow.python.platform import gfile\n",
    "\n",
    "OUTPUTS = [out.op.name for out in model.outputs]\n",
    "print(OUTPUTS)\n",
    "print(OUTPUTS[0])\n",
    "INPUTS = [inp.op.name for inp in model.inputs]\n",
    "print(INPUTS)\n",
    "print(INPUTS[0])\n",
    "\n",
    "# model is used here \n",
    "# K.set_session(tf.Session(graph=model.output.graph)) \n",
    "init = K.tf.global_variables_initializer() \n",
    "K.get_session().run(init)\n",
    "\n",
    "frozen_graph = freeze_session(K.get_session(), output_names=OUTPUTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_now = time.strftime(\"%d-%m-%Y_%H-%M-%S\", time.localtime())\n",
    "filename = \"Magma_frozen_model_\"+time_now+\".pb\"\n",
    "\n",
    "#write the TensorRT model to be used later for inference\n",
    "with gfile.FastGFile(\"./saved_model/\"+filename, 'wb') as f:\n",
    "    f.write(frozen_graph.SerializeToString())\n",
    "print(\"Frozen model is successfully stored!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras to TensorRT\n",
    "![alt text](pictures/Keras_to_TensorRT.png)\n",
    "## c) Optimize the frozen model to TensorRT graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs = [\"dense_2/Softmax\"]\n",
    "# outputs = [\"output_tensor/Softmax\"]\n",
    "\n",
    "# convert (optimize) frozen model to TensorRT model\n",
    "trt_graph = trt.create_inference_graph(\n",
    "    input_graph_def=frozen_graph,# frozen model\n",
    "    outputs=OUTPUTS,\n",
    "    # outputs=outputs,\n",
    "    max_batch_size=2,# specify your max batch size\n",
    "    max_workspace_size_bytes=2*(10**9),# specify the max workspace\n",
    "    precision_mode=\"FP32\") # precision, can be \"FP32\" (32 floating point precision) or \"FP16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_now = time.strftime(\"%d-%m-%Y_%H-%M-%S\", time.localtime())\n",
    "tr_filename = \"TensorRT_Magma_model_\"+time_now+\".pb\"\n",
    "\n",
    "#write the TensorRT model to be used later for inference\n",
    "with gfile.FastGFile(\"./saved_model/\"+tr_filename, 'wb') as f:\n",
    "    f.write(trt_graph.SerializeToString())\n",
    "print(\"TensorRT model is successfully stored!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count how many nodes/operations before and after optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many ops of the original frozen model\n",
    "all_nodes = len([1 for n in frozen_graph.node])\n",
    "print(\"numb. of all_nodes in frozen graph:\", all_nodes)\n",
    "\n",
    "# check how many ops that is converted to TensorRT engine\n",
    "trt_engine_nodes = len([1 for n in trt_graph.node if str(n.op) == 'TRTEngineOp'])\n",
    "print(\"numb. of trt_engine_nodes in TensorRT graph:\", trt_engine_nodes)\n",
    "all_nodes = len([1 for n in trt_graph.node])\n",
    "print(\"numb. of all_nodes in TensorRT graph:\", all_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the original and optimized graphs\n",
    "Using [netron](https://lutzroeder.github.io/netron/), the web application for vitsulaize model graph by upload .pb file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
